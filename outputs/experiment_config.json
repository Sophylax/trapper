{
    "dataset_reader": {
        "type": "squad-clue-extraction"
    },
    "model": {
        "type": "question_answering"
    },
    "args": {
        "type": "default",
        "do_eval": true,
        "do_train": true,
        "evaluation_strategy": "steps",
        "gradient_accumulation_steps": 2,
        "label_names": [
            "start_positions",
            "end_positions"
        ],
        "logging_dir": "/tmp/test_clue_extraction/checkpoints/logs",
        "logging_steps": 500,
        "lr_scheduler_type": "linear",
        "no_cuda": false,
        "num_train_epochs": 3,
        "output_dir": "/tmp/test_clue_extraction/checkpoints",
        "per_device_eval_batch_size": 32,
        "per_device_train_batch_size": 8,
        "result_dir": "/home/cemil/PycharmProjects/question-generation/transformers_wrapper/outputs",
        "save_steps": 500,
        "save_total_limit": 1,
        "warmup_steps": 500
    },
    "data_collator": {
        "type": "clue-extraction"
    },
    "dev_file_path": "/home/cemil/PycharmProjects/question-generation/transformers_wrapper/tests/test_data/squad_qg/dev.json",
    "optimizer": {
        "type": "huggingface_adamw",
        "eps": 1e-06,
        "lr": 5e-05,
        "parameter_groups": [
            [
                [
                    "bias",
                    "LayerNorm\\\\.weight",
                    "layer_norm\\\\.weight"
                ],
                {
                    "weight_decay": 0
                }
            ]
        ],
        "weight_decay": 0.01
    },
    "pretrained_model_name_or_path": "roberta-base",
    "tokenizer": {
        "type": "clue-extraction"
    },
    "train_file_path": "/home/cemil/PycharmProjects/question-generation/transformers_wrapper/tests/test_data/squad_qg/train.json"
}